{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26181423-2985-4bc2-8836-68f24150cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0beb7366-607b-4a18-aa65-b0b76321dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_regex(text):\n",
    "    text = text.replace(\"n't\", \" not\") # deal with contractions\n",
    "    text = re.sub(\"\\'m|\\'ll|\\'s|\\'d\", \" \", text)\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    return re.sub(r'[^\\w\\s\\n\\t]', '', text)\n",
    "\n",
    "def tokenize(text):\n",
    "    text_vec = [e.strip().lower() for e in text.split(\" \")]\n",
    "    text_vec = [e for e in text_vec if e != \"\"]\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46366992-8c29-4a84-8518-e1d1f9a8791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = [\"Dennis+Schwartz\", \"James+Berardinelli\", \"Scott+Renshaw\", \"Steve+Rhodes\"]\n",
    "reviews = []\n",
    "ratings = []\n",
    "for fn in folder_names:\n",
    "    with open(\"data/scaledata/{0}/subj.{1}\".format(fn, fn)) as f:\n",
    "        documents = f.readlines()\n",
    "        reviews.extend([remove_punctuation_regex(d) for d in documents])\n",
    "    with open(\"data/scaledata/{0}/rating.{1}\".format(fn, fn)) as f:\n",
    "        scales = f.readlines()\n",
    "        ratings.extend([float(s.replace(\"\\n\", \"\").strip()) for s in scales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ddf3fd4-d6ee-4b77-aeba-2e2701f82d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus, df_min, df_max_proportion, tfidf_threshold_q):\n",
    "    '''\n",
    "    INPUT:\n",
    "    corpus: a dictionary of dictionaries: each inner dictionary maps each word to its count in the document\n",
    "    df_min: minimum # of document frequency to keep a word in vocabulary\n",
    "    df_max_proportion: maximum document frequency (in proportions) to keep a word in vocabulary\n",
    "    tfidf_threshold_q: the threshold tf-idf quantile to keep a word in a document\n",
    "    OUTPUT:\n",
    "    word_counts_by_document: the updated corpus after fitering by df and tf-idf\n",
    "    vocab_new: list of all unique words in the corpus after filtering\n",
    "    n: total number of words in the updated corpus\n",
    "    '''\n",
    "    word_counts_by_document = {j:Counter(tokenize(rv)) for j,rv in enumerate(corpus)}\n",
    "    document_length = {j:len(rv) for j,rv in enumerate(corpus)}\n",
    "    tf = {}\n",
    "    for k,v in word_counts_by_document.items():\n",
    "        tf[k] = {}\n",
    "        for k2,v2 in v.items():\n",
    "            tf[k][k2] = v2/document_length[k]\n",
    "    df = {}\n",
    "    for k,v in tf.items():\n",
    "        for k2, v2 in v.items():\n",
    "            if v2 > 0:\n",
    "                try:\n",
    "                    df[k2] += 1\n",
    "                except KeyError:\n",
    "                    df[k2] = 1\n",
    "    vocab = set()\n",
    "    for k,v in df.items():\n",
    "        if v/len(reviews) <= df_max_proportion and v >= df_min:\n",
    "            vocab.add(k)\n",
    "    idf = {k:np.log(len(reviews)/v) for k,v in df.items() if k in vocab}\n",
    "    tfidf = {}\n",
    "    for k,v in tm.items():\n",
    "        tfidf[k] = {}\n",
    "        for k2,v2 in v.items():\n",
    "            if k2 in vocab:\n",
    "                tfidf[k][k2] = v2 * idf[k2]\n",
    "    all_tfidf_vals = []\n",
    "    for k,v in tfidf.items():\n",
    "        for k2,v2 in v.items():\n",
    "            all_tfidf_vals.append(v2)\n",
    "    threshold_tfidf = np.quantile(all_tfidf_vals, tfidf_threshold_q)\n",
    "    tfidf_new = {}\n",
    "    for k,v in tfidf.items():\n",
    "        tfidf_new[k] = {}\n",
    "        for k2,v2 in v.items():\n",
    "            if v2 >= threshold_tfidf:\n",
    "                tfidf_new[k][k2] = v2\n",
    "    word_counts_by_document_new = {}\n",
    "    n = 0\n",
    "    vocab_new = set()\n",
    "    for k,v in tfidf_new.items():\n",
    "        word_counts_by_document_new[k] = {}\n",
    "        for k2,v2 in v.items():\n",
    "            n += word_counts_by_document[k][k2]\n",
    "            vocab_new.add(k2)\n",
    "            word_counts_by_document_new[k][k2] = word_counts_by_document[k][k2]\n",
    "    return word_counts_by_document_new, vocab_new, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e83fa4d-cfb8-4006-9024-c058e97a508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_bow, vocab_list, num_of_words = create_bag_of_words(reviews, 5, 0.25, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6f8a44fb-fc54-4f4b-8162-72cad19a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(review_bow, open(\"data/scaledata/cleaned_reviews.pickle\", \"wb\"))\n",
    "pickle.dump(ratings, open(\"data/scaledata/cleaned_ratings.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c329c-8bf7-4ab1-8232-fcdd8cf8b021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
