{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d689ede8-189c-45e1-897b-f3e6b96bbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from text_processing_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c2c6b2-0b35-4692-b6b1-7acfeeef6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ratings = np.array(pickle.load(open(\"data/scaledata/cleaned_ratings.pickle\", \"rb\")))\n",
    "cleaned_reviews = pickle.load(open(\"data/scaledata/cleaned_reviews.pickle\", \"rb\"))\n",
    "vocabulary_dict = pickle.load(open(\"data/scaledata/vocabulary_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382e0a36-c282-47c7-8309-f9026ed56fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4004 1002\n"
     ]
    }
   ],
   "source": [
    "# split the movie reviews data into training/testing parts (80:20)\n",
    "np.random.seed(54321)\n",
    "train_indices = np.random.choice(np.arange(len(cleaned_ratings)), int(len(cleaned_ratings)*0.8), replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(cleaned_ratings)), train_indices)\n",
    "print(len(train_indices), len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53d3519-e58d-475f-a01b-a8ad9a1a3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = convert_bow([cleaned_reviews[i] for i in train_indices])\n",
    "test_bow = convert_bow([cleaned_reviews[i] for i in test_indices])\n",
    "train_y = cleaned_ratings[train_indices]\n",
    "test_y = cleaned_ratings[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f7c96a-37e0-4255-8cdd-6ce1bdb56a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_inference_utils import *\n",
    "from scipy.special import polygamma\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7c2876c-f6cc-487a-88a9-99cd67f7b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI_sLDA_E_Step:\n",
    "    '''\n",
    "    Note: \n",
    "    - E step does not depend on the global hyperparameter xi\n",
    "    - Local variational parameters are updated document-wise\n",
    "    '''\n",
    "    def __init__(self, K, bow, y, alpha, eta, delta, Lambda, epsilon=1e-4):\n",
    "        self.K = K # number of topics\n",
    "        self.bow = bow # dictionaries of arrays, with length D: each array represents the bag of words in the d^th document, with the length of array being N_d\n",
    "        self.doc_len = {d:len(v) for d,v in bow.items()} # number of words within each document\n",
    "        self.D = len(self.bow) # batch_size: number of documents in the minibatch\n",
    "        self.y = y # D-dimensional vector\n",
    "        self.alpha = alpha # K-dimensional vector\n",
    "        self.eta = eta # K-dimensional vector\n",
    "        self.delta = delta # scalar\n",
    "        self.Lambda = Lambda # size: K x V\n",
    "        self.Lambda_rowsum = np.sum(Lambda, axis=1)\n",
    "        self.gamma = {d:np.ones(shape=(K,)) for d in range(self.D)} # initialize local variational parameter gamma (size: D x K)\n",
    "        self.phi = {d:np.empty(shape=(self.doc_len[d], K)) for d in range(self.D)} # initialize local variational parameter phi (for each document, size is N_d x K)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update_gamma(self, d):\n",
    "        # update rule for local variational parameter gamma\n",
    "        sum_phi = np.sum(self.phi[d], axis=0) # K-dimensional\n",
    "        self.gamma[d] = self.alpha + sum_phi\n",
    "\n",
    "    def update_phi_unsupervised(self, d):\n",
    "        # update rule for local variational parameter phi in case y is not observed (prediction mode): same as naive LDA\n",
    "        # can use vectorized operations to update each phi_d\n",
    "        log_phi = polygamma(0, self.Lambda[:, self.bow[d]]).T + polygamma(0, self.gamma[d]) - polygamma(0, self.Lambda_rowsum) # the first term has size N_d x K, the 2nd & 3rd terms are K-dimensional vectors, so broadcasting is applicable\n",
    "        self.phi[d] = exp_normalize_by_row(log_phi) # use log-sum-exp normalization, as the raw values of log_phi could be very negative\n",
    "        \n",
    "    def update_phi_supervised(self, d):\n",
    "        # update rule for local variational parameter phi when y is observed (training mode): Eq (33) of the sLDA paper\n",
    "        N_d = self.doc_len[d]\n",
    "        temp_var_1 = (self.y[d]/N_d/self.delta) * self.eta\n",
    "        temp_var_2 = 1/(2*N_d**2*self.delta)\n",
    "        temp_var_3 = self.eta**2\n",
    "        for j,v in enumerate(self.bow[d]):\n",
    "            log_phi_j = polygamma(0, self.Lambda[:, v]) + polygamma(0, self.gamma[d]) - polygamma(0, self.Lambda_rowsum) # first 2 terms same as the unsupervised case\n",
    "            phi_minus_j = self.phi[d].sum(axis=0) - self.phi[d][j,:] # K-dimensional vector\n",
    "            log_phi_j += temp_var_1 - temp_var_2 * (2*np.dot(self.eta, phi_minus_j)*self.eta + temp_var_3) # Eq (33) of sLDA paper\n",
    "            self.phi[d][j,:] = exp_normalize(log_phi_j) # use log-sum-exp normalization, as the raw values of log_phi_j could be very negative\n",
    "\n",
    "    def coordinate_ascent_training(self, prediction=False):\n",
    "        for d in range(self.D):\n",
    "            self.update_phi_unsupervised(d) # initialize phi without using y: based on values of initial values of gamma\n",
    "        for d in range(self.D):\n",
    "            change_in_gamma = math.inf\n",
    "            while change_in_gamma > self.epsilon: # stopping criteria for convergence: average change in every component of gamma is <= epsilon\n",
    "                if prediction == True:\n",
    "                    self.update_phi_unsupervised(d)\n",
    "                else:\n",
    "                    self.update_phi_supervised(d)\n",
    "                previous_gamma = self.gamma[d].copy()\n",
    "                self.update_gamma(d)\n",
    "                change_in_gamma = np.mean(np.abs(self.gamma[d] - previous_gamma))\n",
    "        \n",
    "        return self.gamma, self.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a065dc5-8c44-4428-882f-e98ab58e9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_inference_utils import *\n",
    "from scipy.special import polygamma, gammaln\n",
    "from scipy.stats import entropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c45c4e81-af54-4cd2-98dd-29f75184a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI_sLDA_M_Step:\n",
    "    '''\n",
    "    The default mode is minibatch natural gradient descent\n",
    "    '''\n",
    "    def __init__(self, K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size, rho=None):\n",
    "        self.K = K # number of topics\n",
    "        self.bow = bow # Bag of words: list of dictionaries, with length D\n",
    "        self.doc_len = {d:len(v) for d,v in bow.items()} # number of words within each document\n",
    "        self.D = len(self.bow) # batch_size: number of documents in the minibatch\n",
    "        self.y = y # D-dimensional vector\n",
    "        self.alpha = alpha # K-dimensional vector\n",
    "        self.new_alpha = None\n",
    "        self.xi = xi # V-dimensional vector\n",
    "        self.new_xi = None\n",
    "        self.eta = eta # K-dimensional vector\n",
    "        self.new_eta = None\n",
    "        self.delta = delta # scalar\n",
    "        self.new_delta = None\n",
    "        self.Lambda = Lambda # global variational parameter Lambda (size: K x V)\n",
    "        self.new_Lambda = None\n",
    "        self.gamma = gamma # local variational parameters gamma from E step (size: D x K)\n",
    "        self.phi = phi # local variational parameters gamma from M step (for each document, size is N_d x K)\n",
    "        self.phi_bar = np.vstack([self.phi[d].mean(axis=0) for d in range(self.D)]) # average phi within each document (size: D x K)\n",
    "        phi_minus_j = {d:(self.phi[d].sum(axis=0) - self.phi[d]) for d in range(self.D)} # for each document, size is N_d x K\n",
    "        self.expect_x_x_t = np.zeros(shape=(K,K)) # size: K x K (only dependent on local variational parameter phi)\n",
    "        for d in range(self.D): # Eq (29) of sLDA paper\n",
    "            N_d = self.doc_len[d]\n",
    "            self.expect_x_x_t += 1/N_d**2 * (self.phi[d].T @ phi_minus_j[d]) # first term of E[Z @ Z^T]\n",
    "            self.expect_x_x_t += 1/N_d**2 * np.diag(self.phi[d].sum(axis=0)) # second term of E[Z @ Z^T]\n",
    "        self.rho = rho\n",
    "        self.corpus_size = corpus_size\n",
    "        self.scale_factor = corpus_size / self.D\n",
    "        \n",
    "    def update_Lambda(self, batch = False):\n",
    "        # update rule for the global variational parameter Lambda: \n",
    "        # depends on local variational parameter phi from the E-step\n",
    "        Lambda_hat = np.zeros_like(self.Lambda) # natural gradient of ELBO w.r.t the variational distribution q(beta | Lambda)\n",
    "        for d in range(self.D):\n",
    "            for wi,v in enumerate(self.bow[d]): # wi is the wi^th word in the d^th document, v is the word's index in the Vocabulary\n",
    "                Lambda_hat[:,v] += self.phi[d][wi,:] # self.phi[d][wi,:] is in fact the variational posterior distribution of topics for the wi^th word in the d^th document\n",
    "        Lambda_hat = self.scale_factor * Lambda_hat # scale based on minibatch size\n",
    "        Lambda_hat += self.xi # same for each variational topic distribution parameter\n",
    "        if batch == False:\n",
    "            self.new_Lambda = stochastic_variational_update(self.Lambda, Lambda_hat, self.rho)\n",
    "        else:\n",
    "            self.Lambda = Lambda_hat\n",
    "        \n",
    "    def update_alpha(self, batch = False):\n",
    "        # update rule for the global hyperparameter alpha:\n",
    "        # depends on local variational parameter gamma from the E-step\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        g = self.D * (polygamma(0, alpha_sum) - polygamma(0, self.alpha))\n",
    "        g += polygamma(0, self.gamma).sum(axis=0) - np.sum(polygamma(0, self.gamma.sum(axis=1))) # gradient of ELBO w.r.t. alpha\n",
    "        g = self.scale_factor * g # scale based on minibatch size\n",
    "        h = -self.corpus_size * polygamma(1, self.alpha) # trigamma\n",
    "        z = self.corpus_size * polygamma(1, alpha_sum) # trigamma\n",
    "        alpha_hat = linear_time_natural_gradient(g, h, z) # compute (the negative scaled) natural gradient of ELBO w.r.t. p(theta_{1:corpur_size} | alpha)\n",
    "        if batch == False:\n",
    "            self.new_alpha = stochastic_hyperparameter_update(self.alpha, alpha_hat, self.rho)\n",
    "        else: # in the batch VI mode, this corresponds to one iteration of New-Raphson algorithm to update alpha\n",
    "            self.new_alpha = self.alpha - alpha_hat\n",
    "\n",
    "    def update_xi(self, batch=False):\n",
    "        # update rule for the global hyperparameter xi:\n",
    "        # depends on global variational parameter Lambda from the previous variational EM iteration\n",
    "        xi_sum = np.sum(self.xi)\n",
    "        g = self.K * (polygamma(0, xi_sum) - polygamma(0, self.xi))\n",
    "        g += polygamma(0, self.Lambda).sum(axis=0) - np.sum(polygamma(0, self.Lambda.sum(axis=1))) # gradient of ELBO w.r.t xi\n",
    "        h = -self.K * polygamma(1, self.xi)\n",
    "        z = self.K * polygamma(1, xi_sum)\n",
    "        xi_hat = linear_time_natural_gradient(g, h, z) # compute the (negative) natural gradient of ELBO w.r.t. p(beta_{1:K} | xi)\n",
    "        if batch == False:\n",
    "            self.new_xi = stochastic_hyperparameter_update(self.xi, xi_hat, self.rho)\n",
    "        else: # in the batch VI mode, this corresponds to one iteration of New-Raphson algorithm to update alpha\n",
    "            self.new_xi = self.xi - xi_hat\n",
    "        \n",
    "    def update_eta_and_delta(self):\n",
    "        # joint update rule for the global hyperparameter (eta, delta) (Gaussian response):\n",
    "        # depends on the local variational parameter phi from the E-step\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar) # K-dimensional vector\n",
    "        expect_x_x_t_times_eta = np.dot(self.expect_x_x_t, self.eta) # K-dimensional vector\n",
    "        y_t_y = np.sum(self.y**2)\n",
    "        temp_var = np.sum(self.eta * (phi_bar_times_y - expect_x_x_t_times_eta/2)) # sum of inner product\n",
    "        g_eta = (1/self.delta)*(phi_bar_times_y - expect_x_x_t_times_eta) # K-dimensional vector\n",
    "        g_delta = -self.D/2/self.delta + 1/2/self.delta**2 * (y_t_y - 2*temp_var)\n",
    "        g = self.scale_factor * np.hstack(g_eta, np.array([g_delta])) # gradient is of K+1 dimensional, scale based on minibatch size\n",
    "        h_11 = -1/self.delta*self.expet_x_x_t\n",
    "        h_21 = -g_eta / self.delta # mixed partial derivatives: K-dimensional vector\n",
    "        h_22 = self.D/2/self.delta**2 - 1/self.delta**3 * (y_t_y - 2*temp_var)\n",
    "        h = np.zeros(shape=(self.K+1, self.K+1))\n",
    "        h[:self.K, :self.K] = h_11\n",
    "        h[self.K, self.K] = h_22\n",
    "        h[self.K, :self.K] = h_21\n",
    "        h[:self.K, self.K] = h_21\n",
    "        h = self.scale_factor * h # (scaled) Hessian is of (K+1) x (K+1) dimensional\n",
    "        h_inv = np.linalg.inv(h)\n",
    "        eta_delta_hat = h_inv @ g # approximated natural gradient of ELBO w.r.t P(Y_{1:corpus_size}|eta, delta)\n",
    "        updated_eta_delta = stochastic_hyperparameter_update(np.hstack(self.eta, np.array([self.delta]), eta_delta_hat, self.rho))\n",
    "        self.new_eta = updated_eta_delta[:self.K]\n",
    "        self.new_delta = updated_eta_delta[self.K]\n",
    "        \n",
    "    def run(self):\n",
    "        # run one full M-step\n",
    "        self.update_Lambda()\n",
    "        self.update_alpha()\n",
    "        self.update_xi()\n",
    "        self.update_eta_and_delta()\n",
    "        return self.new_Lambda, self.new_alpha, self.new_xi, self.new_eta, self.new_delta # output the global parameters to be used in the next iteration of stochastic (minibatch) VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d04e03cc-df81-44cd-b81b-4be1efec61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_VI_sLDA_M_Step(VI_sLDA_M_Step):\n",
    "\n",
    "    def __init__(self, K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size, epsilon): \n",
    "        super().__init__(K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size)\n",
    "        self.epsilon = epsilon # stopping criteria for checking the convergence criteria for Newton-Raphson for alpha and xi\n",
    "        self.elbo = 0 # corpus-level ELBO, which is evaluated at the end of the full M step\n",
    "\n",
    "    def optimize_Lambda(self):\n",
    "        # optimize the global variational parameter Lambda in the M step in batch mode VI:\n",
    "        # it has a closed-form solution\n",
    "        self.update_Lambda(batch=True) # in botch mode VI, the optimization of xi relies on the optimized values of Lambda\n",
    "    \n",
    "    def optimize_alpha(self):\n",
    "        # run a full Newton-Raphson procedure to optimize alpha in the M step\n",
    "        change_in_alpha = math.inf\n",
    "        while change_in_alpha > self.epsilon: # convergence criteria\n",
    "            self.update_alpha(batch=True)\n",
    "            change_in_alpha = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.alpha = self.new_alpha\n",
    "\n",
    "    def optimize_xi(self):\n",
    "        # run a full Newton-Raphson procedure to optimize xi in the M step\n",
    "        change_in_xi = math.inf\n",
    "        while change_in_xi > self.epsilon: # convergence criteria\n",
    "            self.update_xi(batch=True)\n",
    "            change_in_xi = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.xi = self.new_xi\n",
    "\n",
    "    def optimize_eta_and_delta(self):\n",
    "        # optimize in terms of eta and delta has a closed-form solution in batch mode VI\n",
    "        expect_x_x_t_inv = np.linalg.inv(self.expect_x_x_t)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar)\n",
    "        new_eta = np.dot(expect_x_x_t_inv, phi_bar_times_y) \n",
    "        self.delta = 1/self.D * (np.sum(self.y**2) - np.dot(phi_bar_times_y, new_eta))\n",
    "        self.eta = new_eta\n",
    "\n",
    "    def compute_elbo(self):\n",
    "        # the corpus-level ELBO computed at the end of every variational EM iteration, which will be used to determine the stopping time of the entire variational EM\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        temp_var_1 = polygamma(0, self.gamma).T - polygamma(0, self.gamma.sum(axis=1)) # size: K x D\n",
    "        self.elbo += self.D * (gammaln(alpha_sum) - np.sum(gammaln(self.alpha))) + np.sum(np.dot(self.alpha-1, temp_var_1)) # E_q[log p(theta_d | alpha)], summing over d\n",
    "        for d in range(self.D): # E_q[log p(Z_dn | theta_d)] # summing over d and n\n",
    "            self.elbo += np.dot(self.phi[d].sum(axis=0), temp_var_1[:,d])\n",
    "        xi_sum = np.sum(self.xi)\n",
    "        temp_var_2 = polygamma(0, self.Lambda).T - polygamma(0, self.Lambda.sum(axis=1)) # size: V x K\n",
    "        for d in range(self.D): # E_q[log p(w_dn | Z_dn, beta_{1:K}], summing over d and n\n",
    "            for wi,v in enumerate(self.bow[d]): # wi is the wi^th word in the d^th document, v is the word's index in the Vocabulary\n",
    "                self.elbo += np.dot(self.phi[d][wi,:], temp_var_2[v,:])\n",
    "        y_t_y = np.sum(self.y**2)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar) # K-dimensional vector\n",
    "        self.elbo += -self.D/2*np.log(self.delta) - 1/2/self.delta * (y_t_y + np.dot(self.eta, np.dot(self.expect_x_x_t, self.eta)) - 2*np.dot(self.eta, phi_bar_times_y)) # E_q[p(y_d | Z_d, eta, delta)], summing over d\n",
    "        self.elbo += self.K * (gammaln(xi_sum) - np.sum(gammaln(self.xi))) + np.sum(np.dot(self.xi-1, temp_var_2)) # E_q[p(beta_k | lambda_k)], summing over k\n",
    "        temp_var_3 = 0\n",
    "        for d in range(self.D):\n",
    "            temp_var_3 += np.dot(self.gamma[d,:]-1, temp_var_1[:,d])\n",
    "        self.elbo -= np.sum(gammaln(self.gamma.sum(axis=1))) - np.sum(gammaln(self.gamma)) + temp_var_3 # H(q(theta_d | gamma_d)), summing over d\n",
    "        temp_var_4 = 0\n",
    "        for d in range(self.D):\n",
    "            for n in range(self.doc_len[d]):\n",
    "                temp_var_4 -= entropy(self.phi[d][n,:], base=np.exp(1)) # the entropy function could handle 0 * log(0) nicely\n",
    "        self.elbo -= temp_var_4 # H(q(Z_dn | phi_dn)), summing over d and n\n",
    "        temp_var_5 = 0\n",
    "        for k in range(self.K):\n",
    "            temp_var_5 += np.dot(self.Lambda[k,:]-1, temp_var_2[:,k])\n",
    "        self.elbo -= np.sum(gammaln(self.Lambda.sum(axis=1))) - np.sum(gammaln(self.Lambda)) + temp_var_5 # H(q(beta_k | lambda_k)), summing over k\n",
    "        \n",
    "    def run(self):\n",
    "        # override the .run() method in the parent Class\n",
    "        # run the full M step\n",
    "        self.optimize_Lambda()\n",
    "        self.optimize_alpha()\n",
    "        self.optimize_xi()\n",
    "        self.optimize_eta_and_delta()\n",
    "        self.compute_elbo()\n",
    "        return self.Lambda, self.alpha, self.xi, self.eta, self.delta, self.elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7dd6fad0-88a9-4911-94a0-e752e62010ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13fb0800-5699-4d96-9142-befa1d2c0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "K = 12\n",
    "V = len(vocabulary_dict)\n",
    "new_alpha = np.array([1/K]*K)\n",
    "new_xi = np.array([1/V]*V)\n",
    "new_eta = np.linspace(-1,1,K)\n",
    "new_delta = np.var(train_y, ddof=1)\n",
    "new_Lambda = np.abs(np.random.normal(loc=0, scale=0.1, size=K*V)).reshape((K,V)) # initialize Lambda randomly (add a small half-normal distribution to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c455bd-f1d5-4de4-b41c-c3746f3d622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variational EM iteration 0 elbo = -957799.0693075657\n",
      "variational EM iteration 1 elbo = -816562.9403576851\n",
      "variational EM iteration 2 elbo = -761505.1508309841\n",
      "variational EM iteration 3 elbo = -728617.5231866837\n",
      "variational EM iteration 4 elbo = -704664.6793105602\n",
      "variational EM iteration 5 elbo = -686731.4101188183\n",
      "variational EM iteration 6 elbo = -672399.4219100475\n",
      "variational EM iteration 7 elbo = -660322.8902025223\n",
      "variational EM iteration 8 elbo = -650386.9848201275\n",
      "variational EM iteration 9 elbo = -642007.1943261623\n",
      "variational EM iteration 10 elbo = -634863.9637467861\n",
      "variational EM iteration 11 elbo = -628790.2024524212\n"
     ]
    }
   ],
   "source": [
    "for j in range(100):\n",
    "    e_step = VI_sLDA_E_Step(K, {i:train_bow[i] for i in range(400)}, train_y[:400], \n",
    "                            new_alpha, new_eta, new_delta, new_Lambda, \n",
    "                            epsilon=1e-4)\n",
    "    new_gamma, new_phi = e_step.coordinate_ascent_training()\n",
    "    m_step = batch_VI_sLDA_M_Step(K, {i:train_bow[i] for i in range(400)}, train_y[:400],\n",
    "                                  new_alpha, new_xi, new_eta, new_delta, new_Lambda,\n",
    "                                  new_gamma, new_phi,\n",
    "                                  len(train_bow), 1e-4)\n",
    "    new_Lambda, new_alpha, new_xi, new_eta, new_delta, new_elbo = m_step.run()\n",
    "    print(\"variational EM iteration {}: elbo =\".format(j+1), new_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6256cdde-66e5-431c-9b15-15f456456c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca5db90-c35a-4145-a30f-afa1d77b1057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5906236.972399235"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_step.elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9c6d241d-85e6-4f01-bb5c-9bb71247929b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4201,   904, 12777,  1741, 15331,  3633,  2069, 12771,  7869,\n",
       "        8901], dtype=int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (new_Lambda.T / new_Lambda.sum(axis=1))[:,6]\n",
    "np.argsort(test)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc2a4ab-667d-4b74-b737-5d8c7730d0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.436663233779608e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(m_step.Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0576ac0-6787-4a35-9262-b056f6ed6f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673.5425639845672"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(m_step.Lambda[6,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c692c49-aaae-4533-92b5-7b32a95ea3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.056001534354847345"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01 * np.log(0.01) + 0.99 * np.log(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "681dd154-dfe5-44a1-ab74-eb6c9bad0455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056001534354847345"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entropy([0,0,0.01,0.99], base=np.exp(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
