{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9d8e64-bdb3-4674-aba4-cf5bf964f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_inference_utils import *\n",
    "from scipy.special import polygamma, gamma\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d32774-12f5-44f0-b52b-2f21a3e24c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI_sLDA_M_Step:\n",
    "    '''\n",
    "    The default mode is minibatch natural gradient descent\n",
    "    '''\n",
    "    def __init__(self, K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size, rho=None):\n",
    "        self.K = K # number of topics\n",
    "        self.bow = bow # Bag of words: list of dictionaries, with length D\n",
    "        self.doc_len = {d:len(v) for d,v in bow.items()} # number of words within each document\n",
    "        self.D = len(self.bow) # batch_size: number of documents in the minibatch\n",
    "        self.y = y # D-dimensional vector\n",
    "        self.alpha = alpha # K-dimensional vector\n",
    "        self.new_alpha = None\n",
    "        self.xi = xi # V-dimensional vector\n",
    "        self.new_xi = None\n",
    "        self.eta = eta # K-dimensional vector\n",
    "        self.new_eta = None\n",
    "        self.delta = delta # scalar\n",
    "        self.new_delta = None\n",
    "        self.Lambda = Lambda # global variational parameter Lambda (size: K x V)\n",
    "        self.new_Lambda = None\n",
    "        self.gamma = gamma # local variational parameters gamma from E step (size: D x K)\n",
    "        self.phi = phi # local variational parameters gamma from M step (for each document, size is N_d x K)\n",
    "        self.phi_bar = np.vstack([self.phi[d].mean(axis=0) for d in range(self.D)]) # average phi within each document (size: D x K)\n",
    "        phi_minus_j = {d:(self.phi[d].sum(axis=0) - self.phi[d]) for d in range(self.D)} # for each document, size is N_d x K\n",
    "        self.expect_x_x_t = np.zeros(shape=(K,K)) # size: K x K (only dependent on local variational parameter phi)\n",
    "        for d in range(self.D): # Eq (29) of sLDA paper\n",
    "            N_d = self.doc_len[d]\n",
    "            self.expect_x_x_t += 1/N_d**2 * (self.phi[d].T @ phi_minus_j[d]) # first term of E[Z @ Z^T]\n",
    "            self.expect_x_x_t += 1/N_d**2 * np.diag(self.phi[d].sum(axis=0)) # second term of E[Z @ Z^T]\n",
    "        self.rho = rho\n",
    "        self.corpus_size = corpus_size\n",
    "        self.scale_factor = corpus_size / self.D\n",
    "        \n",
    "    def update_Lambda(self, batch = False):\n",
    "        # update rule for the global variational parameter Lambda: \n",
    "        # depends on local variational parameter phi from the E-step\n",
    "        Lambda_hat = np.zeros_like(self.Lambda) # natural gradient of ELBO w.r.t the variational distribution q(beta | Lambda)\n",
    "        for d in range(self.D):\n",
    "            for wi,v in enumerate(self.bow[d]): # wi is the wi^th word in the d^th document, v is the word's index in the Vocabulary\n",
    "                Lambda_hat[:,v] += self.phi[d][wi,:] # self.phi[d][wi,:] is in fact the variational posterior distribution of topics for the wi^th word in the d^th document\n",
    "        Lambda_hat = self.scale_factor * Lambda_hat # scale based on minibatch size\n",
    "        Lambda_hat += self.xi # same for each variational topic distribution parameter\n",
    "        if batch == False:\n",
    "            self.new_Lambda = stochastic_variational_update(self.Lambda, Lambda_hat, self.rho)\n",
    "        else:\n",
    "            self.Lambda = Lambda_hat\n",
    "        \n",
    "    def update_alpha(self, batch = False):\n",
    "        # update rule for the global hyperparameter alpha:\n",
    "        # depends on local variational parameter gamma from the E-step\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        g = self.D * (polygamma(0, alpha_sum) - polygamma(0, self.alpha))\n",
    "        g += polygamma(0, self.gamma).sum(axis=0) - np.sum(polygamma(0, self.gamma.sum(axis=1))) # gradient of ELBO w.r.t. alpha\n",
    "        g = self.scale_factor * g # scale based on minibatch size\n",
    "        h = -self.corpus_size * polygamma(1, self.alpha) # trigamma\n",
    "        z = self.corpus_size * polygamma(1, alpha_sum) # trigamma\n",
    "        alpha_hat = linear_time_natural_gradient(g, h, z) # compute (the negative scaled) natural gradient of ELBO w.r.t. p(theta_{1:corpur_size} | alpha)\n",
    "        if batch == False:\n",
    "            self.new_alpha = stochastic_hyperparameter_update(self.alpha, alpha_hat, self.rho)\n",
    "        else: # in the batch VI mode, this corresponds to one iteration of New-Raphson algorithm to update alpha\n",
    "            self.new_alpha -= alpha_hat\n",
    "\n",
    "    def update_xi(self, batch=False):\n",
    "        # update rule for the global hyperparameter xi:\n",
    "        # depends on global variational parameter Lambda from the previous variational EM iteration\n",
    "        xi_sum = np.sum(self.xi)\n",
    "        g = self.K * (polygamma(0, xi_sum) - polygamma(0, self.xi))\n",
    "        g += polygamma(0, self.Lambda).sum(axis=0) - np.sum(polygamma(0, self.Lambda.sum(axis=1))) # gradient of ELBO w.r.t xi\n",
    "        h = -self.K * polygamma(1, self.xi)\n",
    "        z = self.K * polygamma(1, xi_sum)\n",
    "        xi_hat = linear_time_natural_gradient(g, h, z) # compute the (negative) natural gradient of ELBO w.r.t. p(beta_{1:K} | xi)\n",
    "        if batch == False:\n",
    "            self.new_xi = stochastic_hyperparameter_update(self.xi, xi_hat, self.rho)\n",
    "        else: # in the batch VI mode, this corresponds to one iteration of New-Raphson algorithm to update alpha\n",
    "            self.new_xi -= xi_hat\n",
    "        \n",
    "    def update_eta_and_delta(self):\n",
    "        # joint update rule for the global hyperparameter (eta, delta) (Gaussian response):\n",
    "        # depends on the local variational parameter phi from the E-step\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar) # K-dimensional vector\n",
    "        expect_x_x_t_times_eta = np.dot(self.expect_x_x_t, self.eta) # K-dimensional vector\n",
    "        y_t_y = np.sum(self.y**2)\n",
    "        temp_var = np.sum(self.eta * (phi_bar_times_y - expect_x_x_t_times_eta/2)) # sum of inner product\n",
    "        g_eta = (1/self.delta)*(phi_bar_times_y - expect_x_x_t_times_eta) # K-dimensional vector\n",
    "        g_delta = -self.D/2/self.delta + 1/2/self.delta**2 * (y_t_y - 2*temp_var)\n",
    "        g = self.scale_factor * np.hstack(g_eta, np.array([g_delta])) # gradient is of K+1 dimensional, scale based on minibatch size\n",
    "        h_11 = -1/self.delta*self.expet_x_x_t\n",
    "        h_21 = -g_eta / self.delta # mixed partial derivatives: K-dimensional vector\n",
    "        h_22 = self.D/2/self.delta**2 - 1/self.delta**3 * (y_t_y - 2*temp_var)\n",
    "        h = np.zeros(shape=(self.K+1, self.K+1))\n",
    "        h[:self.K, :self.K] = h_11\n",
    "        h[self.K, self.K] = h_22\n",
    "        h[self.K, :self.K] = h_21\n",
    "        h[:self.K, self.K] = h_21\n",
    "        h = self.scale_factor * h # (scaled) Hessian is of (K+1) x (K+1) dimensional\n",
    "        h_inv = np.linalg.inv(h)\n",
    "        eta_delta_hat = h_inv @ g # approximated natural gradient of ELBO w.r.t P(Y_{1:corpus_size}|eta, delta)\n",
    "        updated_eta_delta = stochastic_hyperparameter_update(np.hstack(self.eta, np.array([self.delta]), eta_delta_hat, self.rho))\n",
    "        self.new_eta = updated_eta_delta[:self.K]\n",
    "        self.new_delta = updated_eta_delta[self.K]\n",
    "        \n",
    "    def run(self):\n",
    "        # run one full M-step\n",
    "        self.update_Lambda()\n",
    "        self.update_alpha()\n",
    "        self.update_xi()\n",
    "        self.update_eta_and_delta()\n",
    "        return self.new_Lambda, self.new_alpha, self.new_xi, self.new_eta, self.new_delta # output the global parameters to be used in the next iteration of stochastic (minibatch) VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40108f13-715f-4731-8d7e-eb5f0a008321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_VI_sLDA_M_Step(VI_sLDA_M_Step):\n",
    "\n",
    "    def __init__(self, K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size, epsilon): \n",
    "        super().__init__(K, bow, y, alpha, xi, eta, delta, Lambda, gamma, phi, corpus_size)\n",
    "        self.epsilon = epsilon # stopping criteria for checking the convergence criteria for Newton-Raphson for alpha and xi\n",
    "        self.elbo = 0 # corpus-level ELBO, which is evaluated at the end of the full M step\n",
    "\n",
    "    def optimize_Lambda(self):\n",
    "        # optimize the global variational parameter Lambda in the M step in batch mode VI:\n",
    "        # it has a closed-form solution\n",
    "        self.update_Lambda(batch=True) # in botch mode VI, the optimization of xi relies on the optimized values of Lambda\n",
    "    \n",
    "    def optimize_alpha(self):\n",
    "        # run a full Newton-Raphson procedure to optimize alpha in the M step\n",
    "        change_in_alpha = math.inf\n",
    "        while chage_in_alpha > self.epsilon: # convergence criteria\n",
    "            self.update_alpha(batch=True)\n",
    "            change_in_alpha = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.alpha = self.new_alpha\n",
    "\n",
    "    def optimize_xi(self):\n",
    "        # run a full Newton-Raphson procedure to optimize xi in the M step\n",
    "        change_in_xi = math.inf\n",
    "        while chage_in_xi > self.epsilon: # convergence criteria\n",
    "            self.update_xi(batch=True)\n",
    "            change_in_xi = np.mean(np.abs(self.new_alpha - self.alpha))\n",
    "            self.xi = self.new_xi\n",
    "\n",
    "    def optimize_eta_and_delta(self):\n",
    "        # optimize in terms of eta and delta has a closed-form solution in batch mode VI\n",
    "        expect_x_x_t_inv = np.linalg.inv(self.expect_x_x_t)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar)\n",
    "        new_eta = np.dot(expect_x_x_t_inv, phi_bar_times_y) \n",
    "        self.delta = 1/self.D * (np.sum(self.y**2) - np.dot(phi_bar_times_y, new_eta))\n",
    "        self.eta = new_eta\n",
    "\n",
    "    def compute_elbo(self):\n",
    "        # the corpus-level ELBO computed at the end of every variational EM iteration, which will be used to determine the stopping time of the entire variational EM\n",
    "        alpha_sum = np.sum(self.alpha)\n",
    "        temp_var_1 = polygamma(0, self.gamma).T - polygamma(0, self.gamma.sum(axis=1)) # size: K x D\n",
    "        self.elbo += self.D * (np.log(gamma(alpha_sum)) - np.sum(np.log(gamma(self.alpha)))) + np.sum(np.dot(self.alpha-1, temp_var_1)) # E_q[log p(theta_d | alpha)], summing over d\n",
    "        for d in range(self.D): # E_q[log p(Z_dn | theta_d)] # summing over d and n\n",
    "            self.elbo += np.dot(self.phi[d].sum(axis=0), temp_var_1[:,d])\n",
    "        xi_sum = np.sum(self.xi)\n",
    "        temp_var_2 = polygamma(0, self.Lambda).T - polygamma(0, self.gamma.sum(axis=1)) # size: V x K\n",
    "        for d in range(self.D): # E_q[log p(w_dn | Z_dn, beta_{1:K}], summing over d and n\n",
    "            for wi,v in enumerate(self.bow[d]): # wi is the wi^th word in the d^th document, v is the word's index in the Vocabulary\n",
    "                self.elbo += np.dot(self.phi[d][wi,:], temp_var_2[v,:])\n",
    "        y_t_y = np.sum(self.y**2)\n",
    "        phi_bar_times_y = np.dot(self.y, self.phi_bar) # K-dimensional vector\n",
    "        self.elbo += -self.D/2*np.log(self.delta) - 1/2/self.delta * (y_t_y + np.dot(self.eta, np.dot(self.expect_x_x_t, self.eta)) - 2*np.dot(self.eta, phi_bar_times_y)) # E_q[p(y_d | Z_d, eta, delta)], summing over d\n",
    "        self.elbo += self.K * (np.log(gamma(xi_sum)) - np.sum(np.log(gamma(self.xi)))) + np.sum(np.dot(self.xi-1, temp_var_2)) # E_q[p(beta_k | lambda_k)], summing over k\n",
    "        temp_var_3 = 0\n",
    "        for d in range(self.D):\n",
    "            temp_var_3 += np.dot(self.gamma[d,:]-1, temp_var_1[:,d])\n",
    "        self.elbo -= np.sum(np.log(gamma(self.gamma.sum(axis=1)))) - np.sum(np.log(gamma(self.gamma))) + temp_var_3 # H(q(theta_d | gamma_d)), summing over d\n",
    "        temp_var_4 = 0\n",
    "        for d in range(self.D):\n",
    "            for n in range(self.doc_len[d]):\n",
    "                temp_var_4 += np.dot(self.phi[d][n,:], np.log(self.phi[d][n,:]))\n",
    "        self.elbo -= temp_var_4 # H(q(Z_dn | phi_dn)), summing over d and n\n",
    "        temp_var_5 = 0\n",
    "        for k in range(self.K):\n",
    "            temp_var_5 += np.dot(self.Lambda[k,:]-1, temp_var_2[:,k])\n",
    "        self.elbo -= np.sum(np.log(gamma(self.Lambda.sum(axis=1)))) - np.sum(np.log(gamma(self.Lambda))) + temp_var_5 # H(q(beta_k | lambda_k)), summing over k\n",
    "        \n",
    "    def run(self):\n",
    "        # override the .run() method in the parent Class\n",
    "        # run the full M step\n",
    "        self.optimize_Lambda()\n",
    "        self.optimize_alpha()\n",
    "        self.optimize_xi()\n",
    "        self.optimize_eta_and_delta()\n",
    "        self.compute_elbo()\n",
    "        return self.Lambda, self.alpha, self.xi, self.eta, self.delta, self.elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e06b0-2a02-4174-aa0b-f58f869a77a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
