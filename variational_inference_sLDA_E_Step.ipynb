{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f037cf5f-da94-44e0-bc12-6cf8ab89762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_inference_utils import *\n",
    "from scipy.special import polygamma\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f29080-e127-4a5c-a7fc-f0573bd67c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9538ec1f-786f-43b4-bcac-5ed1505e851c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e346ecee-a71b-4766-aafa-ae4b05017516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI_sLDA_E_Step:\n",
    "    \n",
    "    def __init__(self, K, bow, y, alpha, eta, delta, Lambda, epsilon=1e-5):\n",
    "        self.K = K # number of topics\n",
    "        self.bow = bow # dictionaries of arrays, with length D: each array represents the bag of words in the d^th document, with the length of array being N_d\n",
    "        self.doc_len = {d:len(v) for d,v in bow.items()} # number of words within each document\n",
    "        self.D = len(self.bow) # batch_size: number of documents in the minibatch\n",
    "        self.y = y # D-dimensional vector\n",
    "        self.alpha = alpha # K-dimensional vector\n",
    "        self.eta = eta # K-dimensional vector\n",
    "        self.delta = delta # scalar\n",
    "        self.Lambda = Lambda # size: K x V\n",
    "        self.Lambda_rowsum = np.sum(Lambda, axis=1)\n",
    "        self.gamma = np.ones(shape=(self.D, K)) # initialize local variational parameter gamma (size: D x K)\n",
    "        self.phi = {d:np.empty(shape=(self.doc_len[d], K)) for d in range(self.D)} # initialize local variational parameter phi (for each document, size is N_d x K)\n",
    "        \n",
    "    def update_gamma(self):\n",
    "        # update rule for local variational parameter gamma\n",
    "        sum_phi = np.vstack([np.sum(v, axis=0) for k,v in self.phi.items()]) # size: D x K\n",
    "        self.gamma = self.alpha + sum_phi # use broadcasting\n",
    "\n",
    "    def update_phi_unsupervised(self):\n",
    "        # update rule for local variational parameter phi in case y is not observed (prediction mode): same as naive LDA\n",
    "        for d in range(self.D): # can use vectorized operations to update each phi_d\n",
    "            log_phi = polygamma(0, self.Lambda[:, self.bow[d]]).T + polygamma(0, self.gamma[d,:]) - polygamma(0, self.Lambda_rowsum) # the first term has size N_d x K, the 2nd & 3rd terms are K-dimensional vectors, so broadcasting is applicable\n",
    "            self.phi[d] = normalize_by_row(np.exp(log_phi))\n",
    "        \n",
    "    def update_phi_supervised(self):\n",
    "        # update rule for local variational parameter phi when y is observed (training mode): Eq (31) of SVI paper\n",
    "        for d in range(self.D):\n",
    "            N_d = self.doc_len[d]\n",
    "            temp_var_1 = (self.y[d]/N_d/self.delta) * self.eta\n",
    "            temp_var_2 = 1/(2*N_d**2*self.delta)\n",
    "            temp_var_3 = self.eta**2\n",
    "            for j,v in enumerate(self.bow[d]):\n",
    "                log_phi_j = polygamma(0, self.Lambda[:, v]) + polygamma(0, self.gamma[d,:]) - polygamma(0, self.Lambda_rowsum) # first 2 terms same as the unsupervised case\n",
    "                phi_minus_j = self.phi[d].sum(axis=0) - self.phi[d][j,:]\n",
    "                log_phi_j += temp_var_1 - temp_var_2 * (2*np.dot(self.eta, phi_minus_j)*self.eta + temp_var_3) # Eq (33) of sLDA paper\n",
    "                self.phi[d][j,:] = normalize_vector(np.exp(log_phi_j)) # dimension of topic space is relatively small, so no need of log-sum-exp normalization\n",
    "\n",
    "    def coordinate_ascent_training(self, prediction=False):\n",
    "        change_in_gamma = math.inf\n",
    "        while change_in_gamma >= self.epsilon: # stopping criteria\n",
    "            if prediction == True:\n",
    "                self.update_phi_unsupervised()\n",
    "            else:\n",
    "                self.update_phi_supervised()\n",
    "            previous_gamma = self.gamma\n",
    "            self.update_gamma()\n",
    "            change_in_gamma = np.mean(np.abs(self.gamma - previous_gamma))\n",
    "        return self.gamma, self.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871ac24-5404-4ad0-9b69-2969754a3269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
